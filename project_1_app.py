import streamlit as st

# ----- NH·∫¨P TH∆Ø VI·ªÜN V√Ä FILE H·ªñ TR·ª¢ C·∫¶N THI·∫æT -----
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
import plotly.express as px

# For EDA and Text Preprocessing
from wordcloud import WordCloud, STOPWORDS
import string
import os
import re
from deep_translator import GoogleTranslator
from langdetect import detect
from underthesea import word_tokenize, pos_tag, sent_tokenize
import emoji

# For Sentiment Analyst
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from nltk.probability import FreqDist
from sklearn.utils import resample

# For Text Clustering
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
from sklearn.decomposition import LatentDirichletAllocation, PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.mixture import GaussianMixture

import joblib
from joblib import dump
from joblib import load

import gdown

import warnings
warnings.filterwarnings('ignore')

# ----- TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N CHO SENTIMENT ANALYSIS -----
def preprocess_review_text(df, col_like, col_suggestion):
    # Load emojicon
    with open('emojicon.txt', 'r', encoding="utf8") as file:
        emoji_dict = {line.split('\t')[0]: line.split('\t')[1] for line in file.read().split('\n') if '\t' in line}

    # Load teencode
    with open('teencode.txt', 'r', encoding="utf8") as file:
        teen_dict = {line.split('\t')[0]: line.split('\t')[1] for line in file.read().split('\n') if '\t' in line}

    # Load English to Vietnamese dictionary
    with open('english-vnmese.txt', 'r', encoding="utf8") as file:
        english_dict = {line.split('\t')[0]: line.split('\t')[1] for line in file.read().split('\n') if '\t' in line}

    # Load wrong word list
    with open('wrong-word-2.txt', 'r', encoding="utf8") as file:
        wrong_lst = file.read().split('\n')

    # Load stopwords
    with open('vietnamese-stopwords.txt', 'r', encoding="utf8") as file:
        stopwords_lst = file.read().split('\n')

    def smart_translate_langdetect(text):
        try:
            lang = detect(text)
            if lang == 'en':
                return GoogleTranslator(source='en', target='vi').translate(text)
            else:
                return text
        except:
            return text

    def process_special_word(text):
        special_words = ['kh√¥ng', 'ch·∫≥ng', 'ch·∫£', 'ch∆∞a', 'thi·∫øu', 'h∆°i']
        new_text = ''
        text_lst = text.split()
        i = 0
        while i < len(text_lst):
            word = text_lst[i]
            if word in special_words and i + 1 < len(text_lst):
                combined = word + '_' + text_lst[i + 1]
                new_text += combined + ' '
                i += 2
            else:
                new_text += word + ' '
                i += 1
        return new_text.strip()

    def loaddicchar():
        uniChars = "√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø"
        unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"
        dic = {}
        char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'.split('|')
        charutf8 = "√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥".split('|')
        for i in range(len(char1252)):
            dic[char1252[i]] = charutf8[i]
        return dic

    def covert_unicode(txt):
        dicchar = loaddicchar()
        return re.sub('|'.join(dicchar.keys()), lambda x: dicchar[x.group()], txt)

    def clean_text_SA(text):
        text = text.lower()
        text = text.replace("'", '')
        text = re.sub(r'\.+', '.', text)
        text = re.sub(r'([a-z]+?)\1+', r'\1', text)
        new_sentence = ''
        for sentence in sent_tokenize(text):
            sentence = ''.join(emoji_dict.get(word, word) + ' ' if word in emoji_dict else word for word in list(sentence))
            sentence = ' '.join(teen_dict.get(word, word) for word in sentence.split())
            pattern = r'(?i)\b[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]+\b'
            sentence = ' '.join(re.findall(pattern, sentence))
            sentence = sentence.replace('.', '')
            lst_word_type = ['A','AB','V','VB','VY','R']
            sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
            new_sentence += sentence + '. '
        text = new_sentence
        english_dict['environment'] = 'moi_truong'
        english_dict['ot'] = 'tang_ca'
        text = ' '.join(english_dict.get(word, word) for word in text.split())
        stopwords_lst.append('cong_ty')
        text = ' '.join(word for word in text.split() if word not in stopwords_lst)
        text = ' '.join(word for word in text.split() if word not in wrong_lst)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    # Process translation
    df[col_like + '_translated'] = df[col_like].apply(smart_translate_langdetect)
    df[col_suggestion + '_translated'] = df[col_suggestion].apply(smart_translate_langdetect)

    # Fill NA
    df[col_like + '_translated'].fillna('na', inplace=True)
    df[col_suggestion + '_translated'].fillna('na', inplace=True)

    # Unicode normalization
    df[col_like + '_translated'] = df[col_like + '_translated'].apply(covert_unicode)
    df[col_suggestion + '_translated'] = df[col_suggestion + '_translated'].apply(covert_unicode)

    # Clean text
    df['like_cleaned'] = df[col_like + '_translated'].apply(clean_text_SA)
    df['suggestion_cleaned'] = df[col_suggestion + '_translated'].apply(clean_text_SA)

    # Combine both
    df['review_cleaned'] = df['like_cleaned'] + ' ' + df['suggestion_cleaned']

    return df[['review_cleaned']]

# ----- Load model c·ªßa Sentiment Analysis -----
# best_rf_pipeline = joblib.load('rf_tfidf_pipeline_sentiment.joblib')
url = "https://drive.google.com/uc?id=1HT6uH8Q-RylS-Fl-9SmrOowRc4xg4ReG"
output = "rf_tfidf_pipeline_sentiment_2.joblib"  # ƒê·ªïi t√™n theo nhu c·∫ßu
if not os.path.exists(output):
    gdown.download(url, output, quiet=False)

# Ki·ªÉm tra dung l∆∞·ª£ng file
if os.path.exists(output):
    size_kb = os.path.getsize(output) / 1024
    # st.write(f"‚úÖ File t·∫£i v·ªÅ: {output} ({size_kb:.2f} KB)")
    if size_kb < 100:  # B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh ng∆∞·ª°ng n√†y
        st.error("‚ö†Ô∏è File m√¥ h√¨nh t·∫£i v·ªÅ c√≥ v·∫ª kh√¥ng ƒë√∫ng ho·∫∑c b·ªã l·ªói.")
    else:
        best_rf_pipeline = joblib.load(output)
else:
    st.error("‚ùå File m√¥ h√¨nh kh√¥ng t·ªìn t·∫°i sau khi t·∫£i.")

# ---- Load data cho text clustering ------
data = pd.read_excel('Reviews.xlsx')
data_TC = pd.read_csv('Reviews_cleaned_for_TC_v2.csv')
data_topic = data_TC.copy()

# ----- Load model c·ªßa Text Clustering -----
pipeline_like = joblib.load('pipeline_like.pkl')
# Tr√≠ch xu·∫•t t·ª´ng b∆∞·ªõc
vectorizer_like = pipeline_like.named_steps['vectorizer']
lda_like = pipeline_like.named_steps['lda']
kmeans_like = pipeline_like.named_steps['kmeans']
# Transform t·ª´ng b∆∞·ªõc
like_vectorizer = vectorizer_like.transform(data_topic['like_cleaned'])
like_topic_dist = lda_like.transform(like_vectorizer)
like_cluster = kmeans_like.predict(like_topic_dist)

pipeline_suggestion = joblib.load('pipeline_suggestion.pkl')
# Tr√≠ch xu·∫•t t·ª´ng b∆∞·ªõc
vectorizer_suggestion = pipeline_suggestion.named_steps['vectorizer']
lda_suggestion = pipeline_suggestion.named_steps['lda']
kmeans_suggestion = pipeline_suggestion.named_steps['kmeans']
# Transform t·ª´ng b∆∞·ªõc
suggestion_vectorizer = vectorizer_suggestion.transform(data_topic['suggestion_cleaned'])
suggestion_topic_dist = lda_suggestion.transform(suggestion_vectorizer)
suggestion_cluster = kmeans_suggestion.predict(suggestion_topic_dist)

data_topic['like_topic'] = like_cluster
data_topic['suggestion_topic'] = suggestion_cluster

# ----- Recommendation Mapping for clustering -----
RECOMMEND_TOPIC_MAP = {
    0: "üè¢ C·∫£i thi·ªán kh√¥ng gian l√†m vi·ªác v√† c∆° s·ªü v·∫≠t ch·∫•t.\nG·ª£i √Ω: n√¢ng c·∫•p vƒÉn ph√≤ng, ch·ªó ng·ªìi, khu v·ª±c ngh·ªâ ng∆°i, thi·∫øt b·ªã.",

    1: "‚öôÔ∏è N√¢ng c·∫•p quy tr√¨nh & ch√≠nh s√°ch n·ªôi b·ªô v√† ho·∫°t ƒë·ªông nh√≥m.\nG·ª£i √Ω: ƒë∆°n gi·∫£n h√≥a th·ªß t·ª•c n·ªôi b·ªô, c·∫£i ti·∫øn h·ªá th·ªëng qu·∫£n l√Ω, tƒÉng minh b·∫°ch.",

    2: "üí∏ C·∫£i thi·ªán ch·∫ø ƒë·ªô tƒÉng ca, l∆∞∆°ng, th∆∞·ªüng v√† ƒë√£i ng·ªô.\nG·ª£i √Ω: xem x√©t ch·∫ø ƒë·ªô tƒÉng ca, ƒëi·ªÅu ch·ªânh l∆∞∆°ng, th∆∞·ªüng theo hi·ªáu su·∫•t, tƒÉng h·ªó tr·ª£ t√†i ch√≠nh."
}

LIKE_TOPIC_MAP = {
    0: "üè¢ Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t",
    1: "üìà C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty",
    2: "üí∞ Ph√∫c l·ª£i & ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp",
}

# T·∫°o DataFrame ch·ªâ ch·ª©a ID c√¥ng ty v√† suggestion_topic
company_suggestions = data_topic[['id', 'like_topic', 'suggestion_topic']]

# Sau ƒë√≥, l·∫•y ra topic c√≥ s·ªë l·∫ßn xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
def get_top_n_suggestions(group, n=2):
  if len(group) == 0:
    return []
  topic_counts = group['suggestion_topic'].value_counts()
  # L·∫•y t·ªëi ƒëa n topic c√≥ t·∫ßn su·∫•t cao nh·∫•t
  top_topics_suggestion = topic_counts.head(n).index.tolist()
  return top_topics_suggestion

def get_top_n_like(group, n=1):
  if len(group) == 0:
    return []
  topic_counts = group['like_topic'].value_counts()
  # L·∫•y t·ªëi ƒëa n topic c√≥ t·∫ßn su·∫•t cao nh·∫•t
  top_topics_like = topic_counts.head(n).index.tolist()
  return top_topics_like

company_top_suggestions = company_suggestions.groupby('id').apply(get_top_n_suggestions).reset_index(name='top_suggestions')
company_top_likes = company_suggestions.groupby('id').apply(get_top_n_like).reset_index(name='top_likes')

# Chuy·ªÉn danh s√°ch c√°c topic suggestions sang t√™n g·ª£i √Ω d·ª±a v√†o mapping
company_top_likes['top_like_names'] = company_top_likes['top_likes'].apply(
    lambda topics: [LIKE_TOPIC_MAP.get(topic, "Kh√¥ng r√µ ch·ªß ƒë·ªÅ") for topic in topics]
)
company_top_suggestions['top_suggestion_names'] = company_top_suggestions['top_suggestions'].apply(
    lambda topics: [RECOMMEND_TOPIC_MAP.get(topic, "Kh√¥ng r√µ ch·ªß ƒë·ªÅ") for topic in topics]
)

### ----- Back-up ------
# H√†m l·∫•y top n topic cho 1 c·ªôt
def get_top_n(group, column, n=1):
    if group.empty:
        return []
    return group[column].value_counts().head(n).index.tolist()

# Nh√≥m theo ID r·ªìi l·∫•y top topic cho c·∫£ 2 c·ªôt trong c√πng 1 apply
company_top_topics = company_suggestions.groupby('id').apply(
    lambda g: pd.Series({
        'top_like': get_top_n(g, 'like_topic'),
        'top_suggestion': get_top_n(g, 'suggestion_topic')
    })
).reset_index()

# Chuy·ªÉn danh s√°ch c√°c topic like v√† suggestions sang t√™n g·ª£i √Ω d·ª±a v√†o mapping
company_top_topics['top_like_names'] = company_top_topics['top_like'].apply(
    lambda topics: [LIKE_TOPIC_MAP.get(topic, "Kh√¥ng r√µ ch·ªß ƒë·ªÅ") for topic in topics]
)

company_top_topics['top_suggestion_names'] = company_top_topics['top_suggestion'].apply(
    lambda topics: [RECOMMEND_TOPIC_MAP.get(topic, "Kh√¥ng r√µ ch·ªß ƒë·ªÅ") for topic in topics]
)

# ----- Streamlit App -----

# Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ ch√≠nh
st.set_page_config(page_title="·ª®ng d·ª•ng Demo", layout="wide")

# Thanh menu b√™n tr√°i (sidebar)
with st.sidebar:
    st.title("Menu")

    # Ch·ªçn trang m·ª•c
    page = st.radio("Ch·ªçn trang", ["1. Gi·ªõi thi·ªáu", "2. Ph√¢n t√≠ch & K·∫øt qu·∫£", "3. Ph√¢n t√≠ch c·∫£m x√∫c", "4. Ph√¢n nh√≥m ƒë√°nh gi√°"])

    # D√≤ng ph√¢n c√°ch
    st.markdown("---")

    # Th√¥ng tin nh√≥m
    st.markdown("**Th√†nh vi√™n nh√≥m:**")
    st.markdown("- Mr. L√™ ƒê·ª©c Anh")
    st.markdown("- Mr. Tr·∫ßn Anh T√∫")

    # Gi√°o vi√™n h∆∞·ªõng d·∫´n
    st.markdown("**GVHD:**")
    st.markdown("- Ms. Khu·∫•t Th√πy Ph∆∞∆°ng")

    # Kho·∫£ng tr·ªëng ƒë·∫©y n·ªôi dung xu·ªëng
    st.markdown("<br><br><br><br><br>", unsafe_allow_html=True)

     # D√≤ng ch·ªØ nh·ªè ·ªü d∆∞·ªõi c√πng
    st.markdown(
        "<div style='font-size: 11px; color: gray; text-align: center;'>"
        "D·ª± √°n t·ªët nghi·ªáp<br>Data Science & Machine Learning<br>TTTH - ƒêH KHTN"
        "</div>",
        unsafe_allow_html=True)

# Hi·ªÉn th·ªã n·ªôi dung theo t·ª´ng trang
if page == "1. Gi·ªõi thi·ªáu":
    # Hi·ªÉn th·ªã banner ITviec
    st.image("banner_itviec_3.jpg", caption='Ngu·ªìn: ITviec', use_container_width=True)
    st.header("1. Gi·ªõi thi·ªáu")

    # Gi·ªõi thi·ªáu ITviec
    st.subheader("V·ªÅ ITviec")
    st.markdown("""
    ITViec l√† n·ªÅn t·∫£ng chuy√™n cung c·∫•p c√°c c∆° h·ªôi vi·ªác l√†m trong lƒ©nh v·ª±c C√¥ng ngh·ªá Th√¥ng tin (IT) h√†ng ƒë·∫ßu Vi·ªát Nam.
    N·ªÅn t·∫£ng n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi√∫p ng∆∞·ªùi d√πng, ƒë·∫∑c bi·ªát l√† c√°c developer, ph√°t tri·ªÉn s·ª± nghi·ªáp m·ªôt c√°ch hi·ªáu qu·∫£.
    Ng∆∞·ªùi d√πng c√≥ th·ªÉ d·ªÖ d√†ng t√¨m ki·∫øm vi·ªác l√†m tr√™n ITViec theo nhi·ªÅu ti√™u ch√≠ kh√°c nhau nh∆∞ k·ªπ nƒÉng, ch·ª©c danh v√† c√¥ng ty.
    B√™n c·∫°nh ƒë√≥, ITViec c√≤n cung c·∫•p nhi·ªÅu t√†i nguy√™n h·ªØu √≠ch h·ªó tr·ª£ ng∆∞·ªùi t√¨m vi·ªác v√† ph√°t tri·ªÉn b·∫£n th√¢n, bao g·ªìm:
    """)
    st.markdown("""
    - **ƒê√°nh gi√° c√¥ng ty**: Gi√∫p ·ª©ng vi√™n c√≥ c√°i nh√¨n t·ªïng quan v·ªÅ m√¥i tr∆∞·ªùng l√†m vi·ªác v√† vƒÉn h√≥a c·ªßa c√°c c√¥ng ty IT.
    - **Blog chuy√™n ng√†nh**: Chia s·∫ª c√°c b√†i vi·∫øt v·ªÅ ki·∫øn th·ª©c chuy√™n m√¥n, k·ªπ nƒÉng m·ªÅm, xu h∆∞·ªõng c√¥ng ngh·ªá v√† c√°c l·ªùi khuy√™n ngh·ªÅ nghi·ªáp h·ªØu √≠ch.
    - **B√°o c√°o l∆∞∆°ng IT**: Cung c·∫•p th√¥ng tin v·ªÅ m·ª©c l∆∞∆°ng tr√™n th·ªã tr∆∞·ªùng, gi√∫p ng∆∞·ªùi d√πng c√≥ c∆° s·ªü ƒë·ªÉ ƒë√†m ph√°n m·ª©c ƒë√£i ng·ªô ph√π h·ª£p.
    """)

    # Gi·ªõi thi·ªáu Dataset
    st.subheader("V·ªÅ b·ªô d·ªØ li·ªáu")
    st.markdown("""
    B·ªô d·ªØ li·ªáu bao g·ªìm **h∆°n 8.000 ƒë√°nh gi√°** t·ª´ c√°c nh√¢n vi√™n v√† c·ª±u nh√¢n vi√™n trong ng√†nh IT t·∫°i Vi·ªát Nam, ƒë∆∞·ª£c thu th·∫≠p t·ª´ ITviec.com.

    C√°c tr∆∞·ªùng ch√≠nh:
    - `What I liked`: Nh·ªØng ƒëi·ªÅu t√≠ch c·ª±c ng∆∞·ªùi ƒë√°nh gi√° c·∫£m nh·∫≠n.
    - `Suggestions for improvement`: G·ª£i √Ω c·∫£i thi·ªán d√†nh cho c√¥ng ty.
    - `Company Mame`, `id`, `Recommend?`, `Overall Rating`, v.v...

    D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ti·∫øng Vi·ªát, chu·∫©n h√≥a v√† l√†m s·∫°ch tr∆∞·ªõc khi √°p d·ª•ng m√¥ h√¨nh h·ªçc m√°y.
    """)

    # M·ª•c ti√™u c·ªßa ·ª©ng d·ª•ng
    st.subheader("M·ª•c ti√™u c·ªßa ·ª©ng d·ª•ng")
    st.markdown("""
    ·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi hai m·ª•c ti√™u ch√≠nh:

    1. **Ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis)**: D·ª± ƒëo√°n c·∫£m x√∫c c·ªßa ng∆∞·ªùi ƒë√°nh gi√° (Good / Neutral / Bad) d·ª±a tr√™n n·ªôi dung h·ªç cung c·∫•p.

    2. **Ph√¢n nh√≥m n·ªôi dung ƒë√°nh gi√° (Information Clustering)**: T·ª± ƒë·ªông ph√¢n lo·∫°i v√† tr·ª±c quan h√≥a c√°c ƒë√°nh gi√° theo ch·ªß ƒë·ªÅ, gi√∫p doanh nghi·ªáp hi·ªÉu r√µ c√°c ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm c·∫ßn c·∫£i thi·ªán.
    """)

elif page == "2. Ph√¢n t√≠ch & K·∫øt qu·∫£":
    st.header("2. Ph√¢n t√≠ch v√† K·∫øt qu·∫£ M√¥ h√¨nh")
    # 1. M√¥ t·∫£ d·ªØ li·ªáu
    st.subheader("2.1 Kh√°m ph√° d·ªØ li·ªáu ban ƒë·∫ßu")
    st.markdown("- S·ªë l∆∞·ª£ng d√≤ng d·ªØ li·ªáu: 8417")
    st.markdown("- S·ªë l∆∞·ª£ng c·ªôt: 12")
    st.markdown("- Tr∆∞·ªùng th√¥ng tin: `What I liked`, `Suggestions for improvement`, `Rating`")
    # Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n
    st.dataframe(data.head())

    col1, col2 = st.columns(2)
    with col1:
      st.markdown("#### L√†m s·∫°ch")
      st.markdown("""
      - **Drop duplicates**: 5 entries  
      - **Drop null values**  
      - **Drop rows where `What I liked` is null**  
      - **Drop rows where `Suggestion for improvement` is null**  
      """)

    with col2:
      st.markdown("#### T·ªïng quan d·ªØ li·ªáu")
      st.markdown("""
        - **`What I liked` feature**  
        - Mean: 237 digits  
        - Max: 6384 digits  
        - **`Suggestion for improvement` feature**  
        - Mean: 138 digits  
        - Max: 3813 digits  
        """)
    st.image("EDA_length.png", caption="ƒê·ªô d√†i k√≠ t·ª±")

    # 2. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
    st.subheader("2.2 Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n")
    
    st.markdown("""
    #### 1. D·ªãch ng√¥n ng·ªØ
    - Ph√°t hi·ªán ng√¥n ng·ªØ v·ªõi `langdetect`
    - D·ªãch ti·∫øng Anh sang ti·∫øng Vi·ªát b·∫±ng `GoogleTranslator`

    #### 2. M√£ h√≥a & Chu·∫©n h√≥a k√Ω t·ª±
    - M√£ h√≥a chu·∫©n `UTF-8`
    - Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n th√†nh ch·ªØ **th∆∞·ªùng**
    - Lo·∫°i b·ªè k√Ω t·ª± **l·∫∑p l·∫°i** (vd: *thi·ªátttt* ‚Üí *thi·ªát*)

    #### 3. X·ª≠ l√Ω Emoji & Teen Code
    - **Sentiment:** Thay emoji b·∫±ng t·ª´ m√¥ t·∫£ (`emojicon.txt`)
    - **Clustering:** Ph√°t hi·ªán v√† x√≥a emoji
    - Chuy·ªÉn teen code sang t·ª´ th√¥ng d·ª•ng (`teencode.txt`)

    #### 4. L√†m s·∫°ch vƒÉn b·∫£n
    - Lo·∫°i b·ªè **d·∫•u c√¢u** v√† **ch·ªØ s·ªë**
    - Chu·∫©n h√≥a **Unicode** t·ª´ c√°c d·∫°ng g√µ l·ªói

    #### 5. POS Tag & T·ª´ ƒë·∫∑c bi·ªát
    - G·∫Øn nh√£n t·ª´ lo·∫°i b·∫±ng `underthesea.pos_tag`
    - Gh√©p t·ª´ ƒë·∫∑c bi·ªát: *kh√¥ng, ch∆∞a, ch·∫£...* ‚Üí *kh√¥ng_t·ªët*, *ch∆∞a_·ªïn*

    #### 6. L·ªçc t·ª´ theo m·ª•c ti√™u
    - **Sentiment:** gi·ªØ t·ª´ lo·∫°i `['A','AB','V','VB','VY','R']`
    - **Clustering:** gi·ªØ th√™m danh t·ª´ `['N', 'Np', 'A', 'AB', 'V', 'VB', 'VY', 'R]`

    #### 7. D·ªãch t·ª´ ƒë∆°n
    - D√πng `english-vnmese.txt` ƒë·ªÉ d·ªãch c√°c t·ª´ ƒë∆°n c√≤n s√≥t

    #### 8. Lo·∫°i b·ªè nhi·ªÖu
    - Xo√° stopwords (`vietnamese-stopwords.txt`)
    - Xo√° t·ª´ sai (`wrong-word-2.txt`)
    - Xo√° kho·∫£ng tr·∫Øng th·ª´a
    """)

    st.markdown("### Word Cloud c·ªßa t·ªáp data cho Sentiment Analysis")
    col3, col4 = st.columns(2)
    with col3:
        st.image("word_cloud_like_sentiment.png", caption="WordCloud - What I liked") #use_container_width=True
    with col4:
        st.image("word_cloud_suggestion_sentiment.png", caption="WordCloud - Suggestions for improvement") #use_container_width=True


    st.markdown("### Word Cloud c·ªßa t·ªáp data cho Information Clustering")
    col5, col6 = st.columns(2)
    with col5:
        st.image("word_cloud_like_cluster.png", caption="WordCloud - What I liked") #use_container_width=True
    with col6:
        st.image("word_cloud_suggestion_cluster.png", caption="WordCloud - Suggestions for improvement") #use_container_width=True

    # 3. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† m√¥ h√¨nh h√≥a
    st.subheader("2.3 M√¥ h√¨nh ph√¢n t√≠ch c·∫£m x√∫c")
    st.markdown("- Ti·∫øn h√†nh **Resample** ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu bi·∫øn `label`")
    st.markdown("- S·ª≠ d·ª•ng m√¥ h√¨nh **Random Forest** v·ªõi ƒë·∫ßu v√†o l√† TF-IDF vector")
    st.markdown("- ƒê·ªô ch√≠nh x√°c: **97.77%**")
    # Hi·ªÉn th·ªã h√¨nh ·∫£nh k·∫øt qu·∫£ c√°c model
    st.image("sentiment_analysis_results.png", caption="Model Comparison")
    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì confusion matrix n·∫øu c√≥
    st.image("confusion_matrix_sentiment_RF.png", caption="Confusion Matrix Random Forest")

    # 4. Ph√¢n t√≠ch ch·ªß ƒë·ªÅ b·∫±ng LDA v√† K Means
    st.subheader("2.4 Ph√¢n t√≠ch ch·ªß ƒë·ªÅ")
    st.markdown("- S·ª≠ d·ª•ng **LDA** v√† 3 m√¥ h√¨nh `KMeans`,  `Agglomerative Clustering`, `Gaussian Mixture`, v√† so s√°nh k·∫øt qu·∫£")
    st.markdown("- K·∫øt qu·∫£ t·ªët nh·∫•t l√† s·ª≠ d·ª•ng **LDA + KMeans** ƒë·ªÉ ph√¢n nh√≥m theo ch·ªß ƒë·ªÅ review")
    st.markdown("- S·ªë l∆∞·ª£ng ch·ªß ƒë·ªÅ (LDA) v√† c·ª•m (KMeans) ƒë·ªÅu l√† 3 cho m·ªói ph·∫ßn `What I liked` v√† `Suggestions for improvement`")

    # Hi·ªÉn th·ªã h√¨nh ·∫£nh bi·ªÉu ƒë·ªì tam gi√°c
    st.markdown("- Silhoutte Score - What I liked: **0.59**")
    st.image("ternary_like.png", caption="Bi·ªÉu ƒë·ªì ph√¢n b·ªï - LIKE")
    st.markdown("- Silhoutte Score - What I liked: **0.64**")
    st.image("ternary_suggestion.png", caption="Bi·ªÉu ƒë·ªì ph·∫©n b·ªï - SUGGESTION")

    # K·∫øt qu·∫£ cluster - LIKE
    st.markdown("### **K·∫øt qu·∫£ cluster 'What I liked'**")
    col7, col8, col9 = st.columns(3)
    # Hi·ªÉn th·ªã ·∫£nh trong t·ª´ng c·ªôt
    with col7:
        st.image("topic_0_like.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 0") # use_container_width=True

    with col8:
        st.image("topic_1_like.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 1") # use_container_width=True

    with col9:
        st.image("topic_2_like.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 2") # use_container_width=True

    # Cluster 0
    st.markdown("#### üè¢ Cluster 0: Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t (ch·ªß ƒë·ªÅ 1)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `vƒÉn_ph√≤ng`, `ƒë·∫πp`, `ƒë·ªôi`, `ph√≤ng`, `m√°y`  
    """)

    # Cluster 1
    st.markdown("#### üìà Cluster 1: C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty (ch·ªß ƒë·ªÅ 2)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `ph√°t_tri·ªÉn`, `ch√≠nh_s√°ch`, `vƒÉn_h√≥a`  
    """)

    # Cluster 2
    st.markdown("#### üí∞ Cluster 2: Ph√∫c l·ª£i, ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp (ch·ªß ƒë·ªÅ 0)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `l∆∞∆°ng`, `tƒÉng_ca`, `ch·∫ø_ƒë·ªô`, `d·ª±_√°n`, `s·∫øp`, `ƒë·ªìng_nghi·ªáp`
    """)

    # K·∫øt qu·∫£ cluster - SUGGESTION
    st.markdown("### **K·∫øt qu·∫£ cluster 'Suggestions for improvement'**")
    col10, col11, col12 = st.columns(3)
    # Hi·ªÉn th·ªã ·∫£nh trong t·ª´ng c·ªôt
    with col10:
        st.image("topic_0_suggestion.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 0") # use_container_width=True

    with col11:
        st.image("topic_1_suggestion.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 1") # use_container_width=True

    with col12:
        st.image("topic_2_suggestion.png", caption="Top 10 keywords - ch·ªß ƒë·ªÅ 2") # use_container_width=True

    # Cluster 0
    st.markdown("#### üè¢ Cluster 0: C·∫£i thi·ªán kh√¥ng gian & c·ªü s·ªü v·∫≠t ch·∫•t (ch·ªß ƒë·ªÅ 0)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `vƒÉn_ph√≤ng`, `ph√≤ng`, `ch·ªó`, `ƒë·ªôi`, `tr∆∞a`, `ho·∫°t_ƒë·ªông`   
    """)

    # Cluster 1
    st.markdown("#### ‚öôÔ∏è Cluster 1: N√¢ng c·∫•p quy tr√¨nh & ch√≠nh s√°ch n·ªôi b·ªô v√† ho·∫°t ƒë·ªông nh√≥m (ch·ªß ƒë·ªÅ 1)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `c·∫£i_thi·ªán`, `ƒë·ªôi`, `ch√≠nh_s√°ch`, `ph√°t_tri·ªÉn`, `d·ª± √°n`, `c·∫£i_thi·ªán` 
    """)

    # Cluster 2
    st.markdown("#### üí∏ Cluster 2: TƒÉng ca, l∆∞∆°ng, th∆∞·ªüng & ƒë√£i ng·ªô (ch·ªß ƒë·ªÅ 2)")
    st.markdown("""
    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `tƒÉng_ca`, `l∆∞∆°ng`, `ti·ªÅn`, `th∆∞·ªüng`, `s·∫øp`, `ch·∫≠m`
    """)

# Sentiment Analysis
elif page == "3. Ph√¢n t√≠ch c·∫£m x√∫c":
    st.header("3. Ph√¢n t√≠ch c·∫£m x√∫c")
    st.write("Nh·∫≠p tay ho·∫∑c t·∫£i file l√™n ƒë·ªÉ d·ª± ƒëo√°n")

    # --- Nh√≥m 1: Nh·∫≠p tay ---
    st.subheader("Nh·∫≠p tay ƒë√°nh gi√°")
    liked_input = st.text_area("What I liked")
    suggestion_input = st.text_area("Suggestions for improvement")

    if st.button("D·ª± ƒëo√°n",  key="predict_manual"):
        if liked_input.strip() == "" and suggestion_input.strip() == "":
            st.warning("Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt tr∆∞·ªùng.")
        else:
            try:
                df_input = pd.DataFrame({
                    "What I liked": [liked_input],
                    "Suggestions for improvement": [suggestion_input]
                })
                df_input = preprocess_review_text(df_input, "What I liked", "Suggestions for improvement")
                preds = best_rf_pipeline.predict(df_input['review_cleaned'])

                label_mapping = {0: 'Bad', 1: 'Neutral', 2: 'Good'}
                pred_label = label_mapping.get(preds[0], "Unknown")

                st.success(f"Sentiment d·ª± ƒëo√°n: **{pred_label}**")
            except Exception as e:
                st.error(f"L·ªói x·∫£y ra: {e}")

    st.markdown("---")

    # --- Nh√≥m 2: Upload file ---
    st.subheader("Upload file Excel (.xlsx) ho·∫∑c CSV (.csv)")
    uploaded_file = st.file_uploader("Ch·ªçn file Excel ho·∫∑c CSV ch·ª©a 2 c·ªôt: 'What I liked' v√† 'Suggestions for improvement'", type=['xlsx', 'csv'])

    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.xlsx'):
                df_upload = pd.read_excel(uploaded_file)
            else:
                df_upload = pd.read_csv(uploaded_file)

            # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
            required_cols = ['What I liked', 'Suggestions for improvement']
            if all(col in df_upload.columns for col in required_cols):
                if st.button("D·ª± ƒëo√°n", key="predict_from_file"):
                    df_cleaned = preprocess_review_text(df_upload, "What I liked", "Suggestions for improvement")
                    preds = best_rf_pipeline.predict(df_cleaned['review_cleaned'])

                    label_mapping = {0: 'Bad', 1: 'Neutral', 2: 'Good'}
                    df_upload['Predicted Sentiment'] = [label_mapping.get(p, "Unknown") for p in preds]

                    st.success("D·ª± ƒëo√°n ho√†n t·∫•t! K·∫øt qu·∫£ hi·ªÉn th·ªã b√™n d∆∞·ªõi:")
                    st.dataframe(df_upload[['What I liked', 'Suggestions for improvement', 'Predicted Sentiment']])

                    # T·∫°o v√† t·∫£i file k·∫øt qu·∫£
                    output_csv = df_upload[['What I liked', 'Suggestions for improvement', 'Predicted Sentiment']].to_csv(index=False).encode('utf-8-sig')
                    st.download_button("T·∫£i xu·ªëng file k·∫øt qu·∫£", data=output_csv, file_name='sentiment_analysis_results.csv', mime='text/csv')
            else:
                st.error(f"File thi·∫øu c·ªôt: {required_cols}")
        except Exception as e:
            st.error(f"L·ªói khi x·ª≠ l√Ω file: {e}")


elif page == "4. Ph√¢n nh√≥m ƒë√°nh gi√°":
    st.header("4. Ph√¢n nh√≥m ƒë√°nh gi√°")
    # st.write("L·ª±a ch·ªçn c√¥ng ty c·∫ßn t√¨m hi·ªÉu")

    # T·∫°o danh s√°ch dropdown "ID - T√™n c√¥ng ty"
    company_options = data_topic[['id', 'Company Name']].drop_duplicates()
    company_options['display_name'] = company_options['id'].astype(str) + " - " + company_options['Company Name']

    selected_display = st.selectbox("Ch·ªçn c√¥ng ty", company_options['display_name'].tolist())

    if selected_display:
        # Tr√≠ch xu·∫•t ID c√¥ng ty t·ª´ chu·ªói ch·ªçn
        company_id_input = int(selected_display.split(" - ")[0])

        try:
            # --- LIKE TRIANGLE ---
            df_like = pd.DataFrame(like_topic_dist, columns=["Topic 0", "Topic 1", "Topic 2"])
            df_like = pd.concat([df_like, data_topic['id']], axis=1)
            df_like = df_like[df_like['id'] == company_id_input]
            df_like["Cluster"] = data_topic['like_topic'].astype(str)

            st.subheader("ƒê√°nh gi√° ph√¢n lo·∫°i c√¥ng ty")
            fig_like = px.scatter_ternary(
                df_like,
                a="Topic 0", b="Topic 1", c="Topic 2",
                color="Cluster",
                size_max=10,
                opacity=0.8,
                title="Bi·ªÉu ƒë·ªì ph√¢n b·ªï ƒë√°nh gi√° 'What I liked'",
                labels={
                    "Topic 0": "Ph√∫c l·ª£i & ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp",
                    "Topic 1": "Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t",
                    "Topic 2": "C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty",
                    "Cluster": "C·ª•m"
                }
            )
            st.plotly_chart(fig_like)

            # --- SUGGESTION TRIANGLE ---
            df_suggestion = pd.DataFrame(suggestion_topic_dist, columns=["Topic 0", "Topic 1", "Topic 2"])
            df_suggestion = pd.concat([df_suggestion, data_topic['id']], axis=1)
            df_suggestion = df_suggestion[df_suggestion['id'] == company_id_input]
            df_suggestion["Cluster"] = data_topic['suggestion_topic'].astype(str)

            # st.subheader("ƒê√°nh gi√° ph√¢n lo·∫°i c√¥ng ty")
            fig_sugg = px.scatter_ternary(
                df_suggestion,
                a="Topic 0", b="Topic 1", c="Topic 2",
                color="Cluster",
                size_max=10,
                opacity=0.8,
                title="Bi·ªÉu ƒë·ªì ph√¢n b·ªï ƒë√°nh gi√° 'Suggestions for improvement'",
                labels={
                    "Topic 0": "C·∫£i thi·ªán kh√¥ng gian & c∆° s·ªü v·∫≠t ch·∫•t",
                    "Topic 1": "Quy tr√¨nh, ch√≠nh s√°ch & teamwork",
                    "Topic 2": "TƒÉng ca, l∆∞∆°ng, th∆∞·ªüng & ƒë√£i ng·ªô",
                    "Cluster": "C·ª•m"
                }
            )
            st.plotly_chart(fig_sugg)

            # --- G·ª¢I √ù C·∫¢I THI·ªÜN ---
            st.subheader("Top ∆∞u ƒëi·ªÉm c·ªßa c√¥ng ty")
            row_like = company_top_likes[company_top_likes['id'] == company_id_input]
            row_suggestion = company_top_suggestions[company_top_suggestions['id'] == company_id_input]

            if not row_like.empty:
                for like in row_like['top_like_names'].iloc[0]:
                    st.markdown(f"**{like}**")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin ∆∞u ƒëi·ªÉm cho c√¥ng ty n√†y.")

            st.subheader("Top ƒë·ªÅ xu·∫•t c·∫£i thi·ªán")
            if not row_suggestion.empty:
                for suggestion in row_suggestion['top_suggestion_names'].iloc[0]:
                    st.markdown(f"**{suggestion}**")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªÅ xu·∫•t cho c√¥ng ty n√†y.")

        except Exception as e:
            st.error(f"C√≥ l·ªói x·∫£y ra: {e}")

