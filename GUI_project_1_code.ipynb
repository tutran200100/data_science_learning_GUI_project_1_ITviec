{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1m4kiZv2WYGldLIZemle-eD_JwliwFe4M","authorship_tag":"ABX9TyNBfiM3glHrU1SmvEP2LP2W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install streamlit\n","!pip install pyngrok\n","!pip install deep_translator\n","!pip install langdetect\n","!pip install underthesea\n","!pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"fz34jP5B91Jm","executionInfo":{"status":"ok","timestamp":1750950573878,"user_tz":-420,"elapsed":48305,"user":{"displayName":"T√∫ Tr·∫ßn","userId":"10158291830435953037"}},"outputId":"8c68c343-bd06-4161-cedc-561df70f2531"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.0)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.44.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.6.15)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n","Requirement already satisfied: underthesea in /usr/local/lib/python3.11/dist-packages (6.8.4)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.2.1)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from underthesea) (0.9.11)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.5.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.6.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\n","Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.4)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.6.15)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.15.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"]}]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/Hoc_tap/Data_Science/7._Do_an_tot_nghiep/GUI_project_1/project_1_app.py\n","import streamlit as st\n","\n","# ----- NH·∫¨P TH∆Ø VI·ªÜN V√Ä FILE H·ªñ TR·ª¢ C·∫¶N THI·∫æT -----\n","import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","\n","# For EDA and Text Preprocessing\n","from wordcloud import WordCloud, STOPWORDS\n","import string\n","import os\n","import re\n","from deep_translator import GoogleTranslator\n","from langdetect import detect\n","from underthesea import word_tokenize, pos_tag, sent_tokenize\n","import emoji\n","\n","# For Sentiment Analyst\n","from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n","from nltk.probability import FreqDist\n","from sklearn.utils import resample\n","\n","# For Text Clustering\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","from sklearn.decomposition import LatentDirichletAllocation, PCA\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.cluster import DBSCAN\n","from sklearn.mixture import GaussianMixture\n","\n","import joblib\n","from joblib import dump\n","from joblib import load\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ----- TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N CHO SENTIMENT ANALYSIS -----\n","def preprocess_review_text(df, col_like, col_suggestion):\n","    # Load emojicon\n","    with open('emojicon.txt', 'r', encoding=\"utf8\") as file:\n","        emoji_dict = {line.split('\\t')[0]: line.split('\\t')[1] for line in file.read().split('\\n') if '\\t' in line}\n","\n","    # Load teencode\n","    with open('teencode.txt', 'r', encoding=\"utf8\") as file:\n","        teen_dict = {line.split('\\t')[0]: line.split('\\t')[1] for line in file.read().split('\\n') if '\\t' in line}\n","\n","    # Load English to Vietnamese dictionary\n","    with open('english-vnmese.txt', 'r', encoding=\"utf8\") as file:\n","        english_dict = {line.split('\\t')[0]: line.split('\\t')[1] for line in file.read().split('\\n') if '\\t' in line}\n","\n","    # Load wrong word list\n","    with open('wrong-word-2.txt', 'r', encoding=\"utf8\") as file:\n","        wrong_lst = file.read().split('\\n')\n","\n","    # Load stopwords\n","    with open('vietnamese-stopwords.txt', 'r', encoding=\"utf8\") as file:\n","        stopwords_lst = file.read().split('\\n')\n","\n","    def smart_translate_langdetect(text):\n","        try:\n","            lang = detect(text)\n","            if lang == 'en':\n","                return GoogleTranslator(source='en', target='vi').translate(text)\n","            else:\n","                return text\n","        except:\n","            return text\n","\n","    def process_special_word(text):\n","        special_words = ['kh√¥ng', 'ch·∫≥ng', 'ch·∫£', 'ch∆∞a', 'thi·∫øu', 'h∆°i']\n","        new_text = ''\n","        text_lst = text.split()\n","        i = 0\n","        while i < len(text_lst):\n","            word = text_lst[i]\n","            if word in special_words and i + 1 < len(text_lst):\n","                combined = word + '_' + text_lst[i + 1]\n","                new_text += combined + ' '\n","                i += 2\n","            else:\n","                new_text += word + ' '\n","                i += 1\n","        return new_text.strip()\n","\n","    def loaddicchar():\n","        uniChars = \"√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø\"\n","        unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n","        dic = {}\n","        char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'.split('|')\n","        charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split('|')\n","        for i in range(len(char1252)):\n","            dic[char1252[i]] = charutf8[i]\n","        return dic\n","\n","    def covert_unicode(txt):\n","        dicchar = loaddicchar()\n","        return re.sub('|'.join(dicchar.keys()), lambda x: dicchar[x.group()], txt)\n","\n","    def clean_text_SA(text):\n","        text = text.lower()\n","        text = text.replace(\"'\", '')\n","        text = re.sub(r'\\.+', '.', text)\n","        text = re.sub(r'([a-z]+?)\\1+', r'\\1', text)\n","        new_sentence = ''\n","        for sentence in sent_tokenize(text):\n","            sentence = ''.join(emoji_dict.get(word, word) + ' ' if word in emoji_dict else word for word in list(sentence))\n","            sentence = ' '.join(teen_dict.get(word, word) for word in sentence.split())\n","            pattern = r'(?i)\\b[a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]+\\b'\n","            sentence = ' '.join(re.findall(pattern, sentence))\n","            sentence = sentence.replace('.', '')\n","            lst_word_type = ['A','AB','V','VB','VY','R']\n","            sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format=\"text\"))))\n","            new_sentence += sentence + '. '\n","        text = new_sentence\n","        english_dict['environment'] = 'moi_truong'\n","        english_dict['ot'] = 'tang_ca'\n","        text = ' '.join(english_dict.get(word, word) for word in text.split())\n","        stopwords_lst.append('cong_ty')\n","        text = ' '.join(word for word in text.split() if word not in stopwords_lst)\n","        text = ' '.join(word for word in text.split() if word not in wrong_lst)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    # Process translation\n","    df[col_like + '_translated'] = df[col_like].apply(smart_translate_langdetect)\n","    df[col_suggestion + '_translated'] = df[col_suggestion].apply(smart_translate_langdetect)\n","\n","    # Fill NA\n","    df[col_like + '_translated'].fillna('na', inplace=True)\n","    df[col_suggestion + '_translated'].fillna('na', inplace=True)\n","\n","    # Unicode normalization\n","    df[col_like + '_translated'] = df[col_like + '_translated'].apply(covert_unicode)\n","    df[col_suggestion + '_translated'] = df[col_suggestion + '_translated'].apply(covert_unicode)\n","\n","    # Clean text\n","    df['like_cleaned'] = df[col_like + '_translated'].apply(clean_text_SA)\n","    df['suggestion_cleaned'] = df[col_suggestion + '_translated'].apply(clean_text_SA)\n","\n","    # Combine both\n","    df['review_cleaned'] = df['like_cleaned'] + ' ' + df['suggestion_cleaned']\n","\n","    return df[['review_cleaned']]\n","\n","# ----- Load model c·ªßa Sentiment Analysis -----\n","best_rf_pipeline = joblib.load('rf_tfidf_pipeline_sentiment.joblib')\n","\n","# ---- Load data cho text clustering ------\n","data = pd.read_excel('Reviews.xlsx')\n","data_TC = pd.read_csv('Reviews_cleaned_for_TC_v2.csv')\n","data_topic = data_TC.copy()\n","\n","# ----- Load model c·ªßa Text Clustering -----\n","pipeline_like = joblib.load('pipeline_like.pkl')\n","# Tr√≠ch xu·∫•t t·ª´ng b∆∞·ªõc\n","vectorizer_like = pipeline_like.named_steps['vectorizer']\n","lda_like = pipeline_like.named_steps['lda']\n","kmeans_like = pipeline_like.named_steps['kmeans']\n","# Transform t·ª´ng b∆∞·ªõc\n","like_vectorizer = vectorizer_like.transform(data_topic['like_cleaned'])\n","like_topic_dist = lda_like.transform(like_vectorizer)\n","like_cluster = kmeans_like.predict(like_topic_dist)\n","\n","pipeline_suggestion = joblib.load('pipeline_suggestion.pkl')\n","# Tr√≠ch xu·∫•t t·ª´ng b∆∞·ªõc\n","vectorizer_suggestion = pipeline_suggestion.named_steps['vectorizer']\n","lda_suggestion = pipeline_suggestion.named_steps['lda']\n","kmeans_suggestion = pipeline_suggestion.named_steps['kmeans']\n","# Transform t·ª´ng b∆∞·ªõc\n","suggestion_vectorizer = vectorizer_suggestion.transform(data_topic['suggestion_cleaned'])\n","suggestion_topic_dist = lda_suggestion.transform(suggestion_vectorizer)\n","suggestion_cluster = kmeans_suggestion.predict(suggestion_topic_dist)\n","\n","data_topic['like_topic'] = like_cluster\n","data_topic['suggestion_topic'] = suggestion_cluster\n","\n","# ----- Recommendation Mapping for clustering -----\n","RECOMMEND_TOPIC_MAP = {\n","    0: \"üè¢ C·∫£i thi·ªán kh√¥ng gian l√†m vi·ªác v√† c∆° s·ªü v·∫≠t ch·∫•t.\\nG·ª£i √Ω: n√¢ng c·∫•p vƒÉn ph√≤ng, ch·ªó ng·ªìi, khu v·ª±c ngh·ªâ ng∆°i, thi·∫øt b·ªã.\",\n","\n","    1: \"‚öôÔ∏è N√¢ng c·∫•p quy tr√¨nh & ch√≠nh s√°ch n·ªôi b·ªô v√† ho·∫°t ƒë·ªông nh√≥m.\\nG·ª£i √Ω: ƒë∆°n gi·∫£n h√≥a th·ªß t·ª•c n·ªôi b·ªô, c·∫£i ti·∫øn h·ªá th·ªëng qu·∫£n l√Ω, tƒÉng minh b·∫°ch.\",\n","\n","    2: \"üí∏ C·∫£i thi·ªán ch·∫ø ƒë·ªô tƒÉng ca, l∆∞∆°ng, th∆∞·ªüng v√† ƒë√£i ng·ªô.\\nG·ª£i √Ω: xem x√©t ch·∫ø ƒë·ªô tƒÉng ca, ƒëi·ªÅu ch·ªânh l∆∞∆°ng, th∆∞·ªüng theo hi·ªáu su·∫•t, tƒÉng h·ªó tr·ª£ t√†i ch√≠nh.\"\n","}\n","\n","LIKE_TOPIC_MAP = {\n","    0: \"üè¢ Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t\",\n","    1: \"üìà C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty\",\n","    2: \"üí∞ Ph√∫c l·ª£i & ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp\",\n","}\n","\n","# T·∫°o DataFrame ch·ªâ ch·ª©a ID c√¥ng ty v√† suggestion_topic\n","company_suggestions = data_topic[['id', 'like_topic', 'suggestion_topic']]\n","\n","# Sau ƒë√≥, l·∫•y ra topic c√≥ s·ªë l·∫ßn xu·∫•t hi·ªán nhi·ªÅu nh·∫•t\n","def get_top_n_suggestions(group, n=2):\n","  if len(group) == 0:\n","    return []\n","  topic_counts = group['suggestion_topic'].value_counts()\n","  # L·∫•y t·ªëi ƒëa n topic c√≥ t·∫ßn su·∫•t cao nh·∫•t\n","  top_topics_suggestion = topic_counts.head(n).index.tolist()\n","  return top_topics_suggestion\n","\n","def get_top_n_like(group, n=1):\n","  if len(group) == 0:\n","    return []\n","  topic_counts = group['like_topic'].value_counts()\n","  # L·∫•y t·ªëi ƒëa n topic c√≥ t·∫ßn su·∫•t cao nh·∫•t\n","  top_topics_like = topic_counts.head(n).index.tolist()\n","  return top_topics_like\n","\n","company_top_suggestions = company_suggestions.groupby('id').apply(get_top_n_suggestions).reset_index(name='top_suggestions')\n","company_top_likes = company_suggestions.groupby('id').apply(get_top_n_like).reset_index(name='top_likes')\n","\n","# Chuy·ªÉn danh s√°ch c√°c topic suggestions sang t√™n g·ª£i √Ω d·ª±a v√†o mapping\n","company_top_likes['top_like_names'] = company_top_likes['top_likes'].apply(\n","    lambda topics: [LIKE_TOPIC_MAP.get(topic, \"Kh√¥ng r√µ ch·ªß ƒë·ªÅ\") for topic in topics]\n",")\n","company_top_suggestions['top_suggestion_names'] = company_top_suggestions['top_suggestions'].apply(\n","    lambda topics: [RECOMMEND_TOPIC_MAP.get(topic, \"Kh√¥ng r√µ ch·ªß ƒë·ªÅ\") for topic in topics]\n",")\n","\n","### ----- Back-up ------\n","# H√†m l·∫•y top n topic cho 1 c·ªôt\n","def get_top_n(group, column, n=1):\n","    if group.empty:\n","        return []\n","    return group[column].value_counts().head(n).index.tolist()\n","\n","# Nh√≥m theo ID r·ªìi l·∫•y top topic cho c·∫£ 2 c·ªôt trong c√πng 1 apply\n","company_top_topics = company_suggestions.groupby('id').apply(\n","    lambda g: pd.Series({\n","        'top_like': get_top_n(g, 'like_topic'),\n","        'top_suggestion': get_top_n(g, 'suggestion_topic')\n","    })\n",").reset_index()\n","\n","# Chuy·ªÉn danh s√°ch c√°c topic like v√† suggestions sang t√™n g·ª£i √Ω d·ª±a v√†o mapping\n","company_top_topics['top_like_names'] = company_top_topics['top_like'].apply(\n","    lambda topics: [LIKE_TOPIC_MAP.get(topic, \"Kh√¥ng r√µ ch·ªß ƒë·ªÅ\") for topic in topics]\n",")\n","\n","company_top_topics['top_suggestion_names'] = company_top_topics['top_suggestion'].apply(\n","    lambda topics: [RECOMMEND_TOPIC_MAP.get(topic, \"Kh√¥ng r√µ ch·ªß ƒë·ªÅ\") for topic in topics]\n",")\n","\n","# ----- Streamlit App -----\n","\n","# Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ ch√≠nh\n","st.set_page_config(page_title=\"·ª®ng d·ª•ng Demo\", layout=\"wide\")\n","\n","# Thanh menu b√™n tr√°i (sidebar)\n","with st.sidebar:\n","    st.title(\"Menu\")\n","\n","    # Ch·ªçn trang m·ª•c\n","    page = st.radio(\"Ch·ªçn trang\", [\"1. Gi·ªõi thi·ªáu\", \"2. Ph√¢n t√≠ch & K·∫øt qu·∫£\", \"3. Ph√¢n t√≠ch c·∫£m x√∫c\", \"4. Ph√¢n nh√≥m ƒë√°nh gi√°\"])\n","\n","    # D√≤ng ph√¢n c√°ch\n","    st.markdown(\"---\")\n","\n","    # Th√¥ng tin nh√≥m\n","    st.markdown(\"**Th√†nh vi√™n nh√≥m:**\")\n","    st.markdown(\"- Mr. L√™ ƒê·ª©c Anh\")\n","    st.markdown(\"- Mr. Tr·∫ßn Anh T√∫\")\n","\n","    # Gi√°o vi√™n h∆∞·ªõng d·∫´n\n","    st.markdown(\"**GVHD:**\")\n","    st.markdown(\"- Ms. Khu·∫•t Th√πy Ph∆∞∆°ng\")\n","\n","    # Kho·∫£ng tr·ªëng ƒë·∫©y n·ªôi dung xu·ªëng\n","    st.markdown(\"<br><br><br><br><br>\", unsafe_allow_html=True)\n","\n","     # D√≤ng ch·ªØ nh·ªè ·ªü d∆∞·ªõi c√πng\n","    st.markdown(\n","        \"<div style='font-size: 11px; color: gray; text-align: center;'>\"\n","        \"D·ª± √°n t·ªët nghi·ªáp<br>Data Science & Machine Learning<br>TTTH - ƒêH KHTN\"\n","        \"</div>\",\n","        unsafe_allow_html=True)\n","\n","# Hi·ªÉn th·ªã n·ªôi dung theo t·ª´ng trang\n","if page == \"1. Gi·ªõi thi·ªáu\":\n","    # Hi·ªÉn th·ªã banner ITviec\n","    st.image(\"banner_itviec_3.jpg\", caption='Ngu·ªìn: ITviec', use_container_width=True)\n","    st.header(\"1. Gi·ªõi thi·ªáu\")\n","\n","    # Gi·ªõi thi·ªáu ITviec\n","    st.subheader(\"V·ªÅ ITviec\")\n","    st.markdown(\"\"\"\n","    ITViec l√† n·ªÅn t·∫£ng chuy√™n cung c·∫•p c√°c c∆° h·ªôi vi·ªác l√†m trong lƒ©nh v·ª±c C√¥ng ngh·ªá Th√¥ng tin (IT) h√†ng ƒë·∫ßu Vi·ªát Nam.\n","    N·ªÅn t·∫£ng n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ gi√∫p ng∆∞·ªùi d√πng, ƒë·∫∑c bi·ªát l√† c√°c developer, ph√°t tri·ªÉn s·ª± nghi·ªáp m·ªôt c√°ch hi·ªáu qu·∫£.\n","    Ng∆∞·ªùi d√πng c√≥ th·ªÉ d·ªÖ d√†ng t√¨m ki·∫øm vi·ªác l√†m tr√™n ITViec theo nhi·ªÅu ti√™u ch√≠ kh√°c nhau nh∆∞ k·ªπ nƒÉng, ch·ª©c danh v√† c√¥ng ty.\n","    B√™n c·∫°nh ƒë√≥, ITViec c√≤n cung c·∫•p nhi·ªÅu t√†i nguy√™n h·ªØu √≠ch h·ªó tr·ª£ ng∆∞·ªùi t√¨m vi·ªác v√† ph√°t tri·ªÉn b·∫£n th√¢n, bao g·ªìm:\n","    \"\"\")\n","    st.markdown(\"\"\"\n","    - **ƒê√°nh gi√° c√¥ng ty**: Gi√∫p ·ª©ng vi√™n c√≥ c√°i nh√¨n t·ªïng quan v·ªÅ m√¥i tr∆∞·ªùng l√†m vi·ªác v√† vƒÉn h√≥a c·ªßa c√°c c√¥ng ty IT.\n","    - **Blog chuy√™n ng√†nh**: Chia s·∫ª c√°c b√†i vi·∫øt v·ªÅ ki·∫øn th·ª©c chuy√™n m√¥n, k·ªπ nƒÉng m·ªÅm, xu h∆∞·ªõng c√¥ng ngh·ªá v√† c√°c l·ªùi khuy√™n ngh·ªÅ nghi·ªáp h·ªØu √≠ch.\n","    - **B√°o c√°o l∆∞∆°ng IT**: Cung c·∫•p th√¥ng tin v·ªÅ m·ª©c l∆∞∆°ng tr√™n th·ªã tr∆∞·ªùng, gi√∫p ng∆∞·ªùi d√πng c√≥ c∆° s·ªü ƒë·ªÉ ƒë√†m ph√°n m·ª©c ƒë√£i ng·ªô ph√π h·ª£p.\n","    \"\"\")\n","\n","    # Gi·ªõi thi·ªáu Dataset\n","    st.subheader(\"V·ªÅ b·ªô d·ªØ li·ªáu\")\n","    st.markdown(\"\"\"\n","    B·ªô d·ªØ li·ªáu bao g·ªìm **h∆°n 8.000 ƒë√°nh gi√°** t·ª´ c√°c nh√¢n vi√™n v√† c·ª±u nh√¢n vi√™n trong ng√†nh IT t·∫°i Vi·ªát Nam, ƒë∆∞·ª£c thu th·∫≠p t·ª´ ITviec.com.\n","\n","    C√°c tr∆∞·ªùng ch√≠nh:\n","    - `What I liked`: Nh·ªØng ƒëi·ªÅu t√≠ch c·ª±c ng∆∞·ªùi ƒë√°nh gi√° c·∫£m nh·∫≠n.\n","    - `Suggestions for improvement`: G·ª£i √Ω c·∫£i thi·ªán d√†nh cho c√¥ng ty.\n","    - `Company Mame`, `id`, `Recommend?`, `Overall Rating`, v.v...\n","\n","    D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ti·∫øng Vi·ªát, chu·∫©n h√≥a v√† l√†m s·∫°ch tr∆∞·ªõc khi √°p d·ª•ng m√¥ h√¨nh h·ªçc m√°y.\n","    \"\"\")\n","\n","    # M·ª•c ti√™u c·ªßa ·ª©ng d·ª•ng\n","    st.subheader(\"M·ª•c ti√™u c·ªßa ·ª©ng d·ª•ng\")\n","    st.markdown(\"\"\"\n","    ·ª®ng d·ª•ng n√†y ƒë∆∞·ª£c x√¢y d·ª±ng v·ªõi hai m·ª•c ti√™u ch√≠nh:\n","\n","    1. **Ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis)**: D·ª± ƒëo√°n c·∫£m x√∫c c·ªßa ng∆∞·ªùi ƒë√°nh gi√° (Good / Neutral / Bad) d·ª±a tr√™n n·ªôi dung h·ªç cung c·∫•p.\n","\n","    2. **Ph√¢n nh√≥m n·ªôi dung ƒë√°nh gi√° (Information Clustering)**: T·ª± ƒë·ªông ph√¢n lo·∫°i v√† tr·ª±c quan h√≥a c√°c ƒë√°nh gi√° theo ch·ªß ƒë·ªÅ, gi√∫p doanh nghi·ªáp hi·ªÉu r√µ c√°c ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm c·∫ßn c·∫£i thi·ªán.\n","    \"\"\")\n","\n","elif page == \"2. Ph√¢n t√≠ch & K·∫øt qu·∫£\":\n","    st.header(\"2. Ph√¢n t√≠ch v√† K·∫øt qu·∫£ M√¥ h√¨nh\")\n","    # 1. M√¥ t·∫£ d·ªØ li·ªáu\n","    st.subheader(\"2.1 Kh√°m ph√° d·ªØ li·ªáu ban ƒë·∫ßu\")\n","    st.markdown(\"- S·ªë l∆∞·ª£ng d√≤ng d·ªØ li·ªáu: 8417\")\n","    st.markdown(\"- S·ªë l∆∞·ª£ng c·ªôt: 12\")\n","    st.markdown(\"- Tr∆∞·ªùng th√¥ng tin: `What I liked`, `Suggestions for improvement`, `Rating`\")\n","    # Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n\n","    st.dataframe(data.head())\n","\n","    col1, col2 = st.columns(2)\n","    with col1:\n","      st.markdown(\"#### L√†m s·∫°ch\")\n","      st.markdown(\"\"\"\n","      - **Drop duplicates**: 5 entries\n","      - **Drop null values**\n","      - **Drop rows where `What I liked` is null**\n","      - **Drop rows where `Suggestion for improvement` is null**\n","      \"\"\")\n","\n","    with col2:\n","      st.markdown(\"#### T·ªïng quan d·ªØ li·ªáu\")\n","      st.markdown(\"\"\"\n","        - **`What I liked` feature**\n","        - Mean: 237 digits\n","        - Max: 6384 digits\n","        - **`Suggestion for improvement` feature**\n","        - Mean: 138 digits\n","        - Max: 3813 digits\n","        \"\"\")\n","    st.image(\"EDA_length.png\", caption=\"ƒê·ªô d√†i k√≠ t·ª±\")\n","\n","    # 2. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n","    st.subheader(\"2.2 Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\")\n","\n","    st.markdown(\"\"\"\n","    #### 1. D·ªãch ng√¥n ng·ªØ\n","    - Ph√°t hi·ªán ng√¥n ng·ªØ v·ªõi `langdetect`\n","    - D·ªãch ti·∫øng Anh sang ti·∫øng Vi·ªát b·∫±ng `GoogleTranslator`\n","\n","    #### 2. M√£ h√≥a & Chu·∫©n h√≥a k√Ω t·ª±\n","    - M√£ h√≥a chu·∫©n `UTF-8`\n","    - Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n th√†nh ch·ªØ **th∆∞·ªùng**\n","    - Lo·∫°i b·ªè k√Ω t·ª± **l·∫∑p l·∫°i** (vd: *thi·ªátttt* ‚Üí *thi·ªát*)\n","\n","    #### 3. X·ª≠ l√Ω Emoji & Teen Code\n","    - **Sentiment:** Thay emoji b·∫±ng t·ª´ m√¥ t·∫£ (`emojicon.txt`)\n","    - **Clustering:** Ph√°t hi·ªán v√† x√≥a emoji\n","    - Chuy·ªÉn teen code sang t·ª´ th√¥ng d·ª•ng (`teencode.txt`)\n","\n","    #### 4. L√†m s·∫°ch vƒÉn b·∫£n\n","    - Lo·∫°i b·ªè **d·∫•u c√¢u** v√† **ch·ªØ s·ªë**\n","    - Chu·∫©n h√≥a **Unicode** t·ª´ c√°c d·∫°ng g√µ l·ªói\n","\n","    #### 5. POS Tag & T·ª´ ƒë·∫∑c bi·ªát\n","    - G·∫Øn nh√£n t·ª´ lo·∫°i b·∫±ng `underthesea.pos_tag`\n","    - Gh√©p t·ª´ ƒë·∫∑c bi·ªát: *kh√¥ng, ch∆∞a, ch·∫£...* ‚Üí *kh√¥ng_t·ªët*, *ch∆∞a_·ªïn*\n","\n","    #### 6. L·ªçc t·ª´ theo m·ª•c ti√™u\n","    - **Sentiment:** gi·ªØ t·ª´ lo·∫°i `['A','AB','V','VB','VY','R']`\n","    - **Clustering:** gi·ªØ th√™m danh t·ª´ `['N', 'Np', 'A', 'AB', 'V', 'VB', 'VY', 'R]`\n","\n","    #### 7. D·ªãch t·ª´ ƒë∆°n\n","    - D√πng `english-vnmese.txt` ƒë·ªÉ d·ªãch c√°c t·ª´ ƒë∆°n c√≤n s√≥t\n","\n","    #### 8. Lo·∫°i b·ªè nhi·ªÖu\n","    - Xo√° stopwords (`vietnamese-stopwords.txt`)\n","    - Xo√° t·ª´ sai (`wrong-word-2.txt`)\n","    - Xo√° kho·∫£ng tr·∫Øng th·ª´a\n","    \"\"\")\n","\n","    st.markdown(\"### Word Cloud c·ªßa t·ªáp data cho Sentiment Analysis\")\n","    col3, col4 = st.columns(2)\n","    with col3:\n","        st.image(\"word_cloud_like_sentiment.png\", caption=\"WordCloud - What I liked\") #use_container_width=True\n","    with col4:\n","        st.image(\"word_cloud_suggestion_sentiment.png\", caption=\"WordCloud - Suggestions for improvement\") #use_container_width=True\n","\n","\n","    st.markdown(\"### Word Cloud c·ªßa t·ªáp data cho Information Clustering\")\n","    col5, col6 = st.columns(2)\n","    with col5:\n","        st.image(\"word_cloud_like_cluster.png\", caption=\"WordCloud - What I liked\") #use_container_width=True\n","    with col6:\n","        st.image(\"word_cloud_suggestion_cluster.png\", caption=\"WordCloud - Suggestions for improvement\") #use_container_width=True\n","\n","    # 3. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† m√¥ h√¨nh h√≥a\n","    st.subheader(\"2.3 M√¥ h√¨nh ph√¢n t√≠ch c·∫£m x√∫c\")\n","    st.markdown(\"- S·ª≠ d·ª•ng m√¥ h√¨nh **Random Forest** v·ªõi ƒë·∫ßu v√†o l√† TF-IDF vector\")\n","    st.markdown(\"- ƒê·ªô ch√≠nh x√°c: **97.77%**\")\n","    # Hi·ªÉn th·ªã h√¨nh ·∫£nh k·∫øt qu·∫£ c√°c model\n","    st.image(\"sentiment_analysis_results.png\", caption=\"Model Comparison\")\n","    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì confusion matrix n·∫øu c√≥\n","    st.image(\"confusion_matrix_sentiment_RF.png\", caption=\"Confusion Matrix Random Forest\")\n","\n","    # 4. Ph√¢n t√≠ch ch·ªß ƒë·ªÅ b·∫±ng LDA v√† K Means\n","    st.subheader(\"2.4 Ph√¢n t√≠ch ch·ªß ƒë·ªÅ\")\n","    st.markdown(\"- S·ª≠ d·ª•ng **LDA + KMeans** ƒë·ªÉ ph√¢n nh√≥m theo ch·ªß ƒë·ªÅ review\")\n","    st.markdown(\"- S·ªë l∆∞·ª£ng ch·ªß ƒë·ªÅ (LDA) v√† c·ª•m (KMeans) ƒë·ªÅu l√† 3 cho m·ªói ph·∫ßn `What I liked` v√† `Suggestion`\")\n","\n","    # Hi·ªÉn th·ªã h√¨nh ·∫£nh bi·ªÉu ƒë·ªì tam gi√°c\n","    st.markdown(\"- Silhoutte Score - What I liked: **0.59**\")\n","    st.image(\"ternary_like.png\", caption=\"Bi·ªÉu ƒë·ªì ph√¢n b·ªï - LIKE\")\n","    st.markdown(\"- Silhoutte Score - What I liked: **0.64**\")\n","    st.image(\"ternary_suggestion.png\", caption=\"Bi·ªÉu ƒë·ªì ph·∫©n b·ªï - SUGGESTION\")\n","\n","    # K·∫øt qu·∫£ cluster - LIKE\n","    st.markdown(\"### **K·∫øt qu·∫£ cluster 'What I liked'**\")\n","    col7, col8, col9 = st.columns(3)\n","    # Hi·ªÉn th·ªã ·∫£nh trong t·ª´ng c·ªôt\n","    with col7:\n","        st.image(\"topic_0_like.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 0\") # use_container_width=True\n","\n","    with col8:\n","        st.image(\"topic_1_like.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 1\") # use_container_width=True\n","\n","    with col9:\n","        st.image(\"topic_2_like.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 2\") # use_container_width=True\n","\n","    # Cluster 0\n","    st.markdown(\"#### üè¢ Cluster 0: Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t (ch·ªß ƒë·ªÅ 1)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `vƒÉn_ph√≤ng`, `ƒë·∫πp`, `ƒë·ªôi`, `ph√≤ng`, `m√°y`\n","    \"\"\")\n","\n","    # Cluster 1\n","    st.markdown(\"#### üìà Cluster 1: C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty (ch·ªß ƒë·ªÅ 2)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `ph√°t_tri·ªÉn`, `ch√≠nh_s√°ch`, `vƒÉn_h√≥a`\n","    \"\"\")\n","\n","    # Cluster 2\n","    st.markdown(\"#### üí∞ Cluster 2: Ph√∫c l·ª£i, ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp (ch·ªß ƒë·ªÅ 0)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `l∆∞∆°ng`, `tƒÉng_ca`, `ch·∫ø_ƒë·ªô`, `d·ª±_√°n`, `s·∫øp`, `ƒë·ªìng_nghi·ªáp`\n","    \"\"\")\n","\n","    # K·∫øt qu·∫£ cluster - SUGGESTION\n","    st.markdown(\"### **K·∫øt qu·∫£ cluster 'Suggestions to improve'**\")\n","    col10, col11, col12 = st.columns(3)\n","    # Hi·ªÉn th·ªã ·∫£nh trong t·ª´ng c·ªôt\n","    with col10:\n","        st.image(\"topic_0_suggestion.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 0\") # use_container_width=True\n","\n","    with col11:\n","        st.image(\"topic_1_suggestion.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 1\") # use_container_width=True\n","\n","    with col12:\n","        st.image(\"topic_2_suggestion.png\", caption=\"Top 10 keywords - ch·ªß ƒë·ªÅ 2\") # use_container_width=True\n","\n","    # Cluster 0\n","    st.markdown(\"#### üè¢ Cluster 0: C·∫£i thi·ªán kh√¥ng gian & c·ªü s·ªü v·∫≠t ch·∫•t (ch·ªß ƒë·ªÅ 0)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `vƒÉn_ph√≤ng`, `ph√≤ng`, `ch·ªó`, `ƒë·ªôi`, `tr∆∞a`, `ho·∫°t_ƒë·ªông`\n","    \"\"\")\n","\n","    # Cluster 1\n","    st.markdown(\"#### ‚öôÔ∏è Cluster 1: N√¢ng c·∫•p quy tr√¨nh & ch√≠nh s√°ch n·ªôi b·ªô v√† ho·∫°t ƒë·ªông nh√≥m (ch·ªß ƒë·ªÅ 1)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `c·∫£i_thi·ªán`, `ƒë·ªôi`, `ch√≠nh_s√°ch`, `ph√°t_tri·ªÉn`, `d·ª± √°n`, `c·∫£i_thi·ªán`\n","    \"\"\")\n","\n","    # Cluster 2\n","    st.markdown(\"#### üí∏ Cluster 2: TƒÉng ca, l∆∞∆°ng, th∆∞·ªüng & ƒë√£i ng·ªô (ch·ªß ƒë·ªÅ 2)\")\n","    st.markdown(\"\"\"\n","    **T·ª´ kh√≥a ti√™u bi·ªÉu:** `tƒÉng_ca`, `l∆∞∆°ng`, `ti·ªÅn`, `th∆∞·ªüng`, `s·∫øp`, `ch·∫≠m`\n","    \"\"\")\n","\n","# Sentiment Analysis\n","elif page == \"3. Ph√¢n t√≠ch c·∫£m x√∫c\":\n","    st.header(\"3. Ph√¢n t√≠ch c·∫£m x√∫c\")\n","    st.write(\"Nh·∫≠p tay ho·∫∑c t·∫£i file l√™n ƒë·ªÉ d·ª± ƒëo√°n\")\n","\n","    # --- Nh√≥m 1: Nh·∫≠p tay ---\n","    st.subheader(\"Nh·∫≠p tay ƒë√°nh gi√°\")\n","    liked_input = st.text_area(\"What I liked\")\n","    suggestion_input = st.text_area(\"Suggestions for improvement\")\n","\n","    if st.button(\"D·ª± ƒëo√°n\",  key=\"predict_manual\"):\n","        if liked_input.strip() == \"\" and suggestion_input.strip() == \"\":\n","            st.warning(\"Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt tr∆∞·ªùng.\")\n","        else:\n","            try:\n","                df_input = pd.DataFrame({\n","                    \"What I liked\": [liked_input],\n","                    \"Suggestions for improvement\": [suggestion_input]\n","                })\n","                df_input = preprocess_review_text(df_input, \"What I liked\", \"Suggestions for improvement\")\n","                preds = best_rf_pipeline.predict(df_input['review_cleaned'])\n","\n","                label_mapping = {0: 'Bad', 1: 'Neutral', 2: 'Good'}\n","                pred_label = label_mapping.get(preds[0], \"Unknown\")\n","\n","                st.success(f\"Sentiment d·ª± ƒëo√°n: **{pred_label}**\")\n","            except Exception as e:\n","                st.error(f\"L·ªói x·∫£y ra: {e}\")\n","\n","    st.markdown(\"---\")\n","\n","    # --- Nh√≥m 2: Upload file ---\n","    st.subheader(\"Upload file Excel (.xlsx) ho·∫∑c CSV (.csv)\")\n","    uploaded_file = st.file_uploader(\"Ch·ªçn file Excel ho·∫∑c CSV ch·ª©a 2 c·ªôt: 'What I liked' v√† 'Suggestions for improvement'\", type=['xlsx', 'csv'])\n","\n","    if uploaded_file is not None:\n","        try:\n","            if uploaded_file.name.endswith('.xlsx'):\n","                df_upload = pd.read_excel(uploaded_file)\n","            else:\n","                df_upload = pd.read_csv(uploaded_file)\n","\n","            # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt\n","            required_cols = ['What I liked', 'Suggestions for improvement']\n","            if all(col in df_upload.columns for col in required_cols):\n","                if st.button(\"D·ª± ƒëo√°n\", key=\"predict_from_file\"):\n","                    df_cleaned = preprocess_review_text(df_upload, \"What I liked\", \"Suggestions for improvement\")\n","                    preds = best_rf_pipeline.predict(df_cleaned['review_cleaned'])\n","\n","                    label_mapping = {0: 'Bad', 1: 'Neutral', 2: 'Good'}\n","                    df_upload['Predicted Sentiment'] = [label_mapping.get(p, \"Unknown\") for p in preds]\n","\n","                    st.success(\"D·ª± ƒëo√°n ho√†n t·∫•t! K·∫øt qu·∫£ hi·ªÉn th·ªã b√™n d∆∞·ªõi:\")\n","                    st.dataframe(df_upload[['What I liked', 'Suggestions for improvement', 'Predicted Sentiment']])\n","\n","                    # T·∫°o v√† t·∫£i file k·∫øt qu·∫£\n","                    output_csv = df_upload[['What I liked', 'Suggestions for improvement', 'Predicted Sentiment']].to_csv(index=False).encode('utf-8-sig')\n","                    st.download_button(\"T·∫£i xu·ªëng file k·∫øt qu·∫£\", data=output_csv, file_name='sentiment_analysis_results.csv', mime='text/csv')\n","            else:\n","                st.error(f\"File thi·∫øu c·ªôt: {required_cols}\")\n","        except Exception as e:\n","            st.error(f\"L·ªói khi x·ª≠ l√Ω file: {e}\")\n","\n","\n","elif page == \"4. Ph√¢n nh√≥m ƒë√°nh gi√°\":\n","    st.header(\"4. Ph√¢n nh√≥m ƒë√°nh gi√°\")\n","    # st.write(\"L·ª±a ch·ªçn c√¥ng ty c·∫ßn t√¨m hi·ªÉu\")\n","\n","    # T·∫°o danh s√°ch dropdown \"ID - T√™n c√¥ng ty\"\n","    company_options = data_topic[['id', 'Company Name']].drop_duplicates()\n","    company_options['display_name'] = company_options['id'].astype(str) + \" - \" + company_options['Company Name']\n","\n","    selected_display = st.selectbox(\"Ch·ªçn c√¥ng ty\", company_options['display_name'].tolist())\n","\n","    if selected_display:\n","        # Tr√≠ch xu·∫•t ID c√¥ng ty t·ª´ chu·ªói ch·ªçn\n","        company_id_input = int(selected_display.split(\" - \")[0])\n","\n","        try:\n","            # --- LIKE TRIANGLE ---\n","            df_like = pd.DataFrame(like_topic_dist, columns=[\"Topic 0\", \"Topic 1\", \"Topic 2\"])\n","            df_like = pd.concat([df_like, data_topic['id']], axis=1)\n","            df_like = df_like[df_like['id'] == company_id_input]\n","            df_like[\"Cluster\"] = data_topic['like_topic'].astype(str)\n","\n","            st.subheader(\"ƒê√°nh gi√° ph√¢n lo·∫°i c√¥ng ty\")\n","            fig_like = px.scatter_ternary(\n","                df_like,\n","                a=\"Topic 0\", b=\"Topic 1\", c=\"Topic 2\",\n","                color=\"Cluster\",\n","                size_max=10,\n","                opacity=0.8,\n","                title=\"Bi·ªÉu ƒë·ªì ph√¢n b·ªï ƒë√°nh gi√° 'What I liked'\",\n","                labels={\n","                    \"Topic 0\": \"Ph√∫c l·ª£i & ƒê√£i ng·ªô & ƒê·ªìng nghi·ªáp\",\n","                    \"Topic 1\": \"Kh√¥ng gian l√†m vi·ªác & C∆° s·ªü v·∫≠t ch·∫•t\",\n","                    \"Topic 2\": \"C∆° h·ªôi ph√°t tri·ªÉn & VƒÉn h√≥a c√¥ng ty\",\n","                    \"Cluster\": \"C·ª•m\"\n","                }\n","            )\n","            st.plotly_chart(fig_like)\n","\n","            # --- SUGGESTION TRIANGLE ---\n","            df_suggestion = pd.DataFrame(suggestion_topic_dist, columns=[\"Topic 0\", \"Topic 1\", \"Topic 2\"])\n","            df_suggestion = pd.concat([df_suggestion, data_topic['id']], axis=1)\n","            df_suggestion = df_suggestion[df_suggestion['id'] == company_id_input]\n","            df_suggestion[\"Cluster\"] = data_topic['suggestion_topic'].astype(str)\n","\n","            # st.subheader(\"ƒê√°nh gi√° ph√¢n lo·∫°i c√¥ng ty\")\n","            fig_sugg = px.scatter_ternary(\n","                df_suggestion,\n","                a=\"Topic 0\", b=\"Topic 1\", c=\"Topic 2\",\n","                color=\"Cluster\",\n","                size_max=10,\n","                opacity=0.8,\n","                title=\"Bi·ªÉu ƒë·ªì ph√¢n b·ªï ƒë√°nh gi√° 'Suggestions for improvement'\",\n","                labels={\n","                    \"Topic 0\": \"C·∫£i thi·ªán kh√¥ng gian & c∆° s·ªü v·∫≠t ch·∫•t\",\n","                    \"Topic 1\": \"Quy tr√¨nh, ch√≠nh s√°ch & teamwork\",\n","                    \"Topic 2\": \"TƒÉng ca, l∆∞∆°ng, th∆∞·ªüng & ƒë√£i ng·ªô\",\n","                    \"Cluster\": \"C·ª•m\"\n","                }\n","            )\n","            st.plotly_chart(fig_sugg)\n","\n","            # --- G·ª¢I √ù C·∫¢I THI·ªÜN ---\n","            st.subheader(\"Top ∆∞u ƒëi·ªÉm c·ªßa c√¥ng ty\")\n","            row_like = company_top_likes[company_top_likes['id'] == company_id_input]\n","            row_suggestion = company_top_suggestions[company_top_suggestions['id'] == company_id_input]\n","\n","            if not row_like.empty:\n","                for like in row_like['top_like_names'].iloc[0]:\n","                    st.markdown(f\"**{like}**\")\n","            else:\n","                st.warning(\"Kh√¥ng t√¨m th·∫•y th√¥ng tin ∆∞u ƒëi·ªÉm cho c√¥ng ty n√†y.\")\n","\n","            st.subheader(\"Top ƒë·ªÅ xu·∫•t c·∫£i thi·ªán\")\n","            if not row_suggestion.empty:\n","                for suggestion in row_suggestion['top_suggestion_names'].iloc[0]:\n","                    st.markdown(f\"**{suggestion}**\")\n","            else:\n","                st.warning(\"Kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªÅ xu·∫•t cho c√¥ng ty n√†y.\")\n","\n","        except Exception as e:\n","            st.error(f\"C√≥ l·ªói x·∫£y ra: {e}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Syi6Q-jknXwL","executionInfo":{"status":"ok","timestamp":1750950574096,"user_tz":-420,"elapsed":215,"user":{"displayName":"T√∫ Tr·∫ßn","userId":"10158291830435953037"}},"outputId":"09700f16-bfd7-4ab4-cab0-dc91765f4273"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/Hoc_tap/Data_Science/7._Do_an_tot_nghiep/GUI_project_1/project_1_app.py\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Hoc_tap/Data_Science/7._Do_an_tot_nghiep/GUI_project_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlkNuGu6FRJ2","executionInfo":{"status":"ok","timestamp":1750950574107,"user_tz":-420,"elapsed":8,"user":{"displayName":"T√∫ Tr·∫ßn","userId":"10158291830435953037"}},"outputId":"053e2b72-f9a6-4961-c2d5-7c613420cfe9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Hoc_tap/Data_Science/7._Do_an_tot_nghiep/GUI_project_1\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","import threading\n","\n","ngrok.set_auth_token('2ywMgs25RHcQ6kXPwb4aBivyJfm_3LE4tutg6aUFhV6Y2WzmJ')\n","\n","# Kh·ªüi t·∫°o c·ªïng cho Streamlit\n","port = 8501\n","public_url = ngrok.connect(port)\n","print(f'Ngrok URL: {public_url}')\n","\n","# Ch·∫°y Streamlit trong thread ph·ª•\n","def run():\n","    !streamlit run project_1_app.py --server.port {port} --server.enableCORS false\n","\n","thread = threading.Thread(target=run)\n","thread.start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnCuIER2C0aI","executionInfo":{"status":"ok","timestamp":1750950574475,"user_tz":-420,"elapsed":367,"user":{"displayName":"T√∫ Tr·∫ßn","userId":"10158291830435953037"}},"outputId":"a2adde76-aa46-40fa-f29c-116c5774676f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Ngrok URL: NgrokTunnel: \"https://6aa4-34-106-94-203.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]}]}